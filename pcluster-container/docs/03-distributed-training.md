# 3. ë¶„ì‚° í•™ìŠµ ì‹¤í–‰

> ğŸ’¡ **ëª©í‘œ:** ì»¨í…Œì´ë„ˆ ê¸°ë°˜ ë¶„ì‚° í•™ìŠµ ì‘ì—…ì„ Slurmìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.

â±ï¸ **ì˜ˆìƒ ì†Œìš” ì‹œê°„:** 40-60ë¶„

## ëª©ì°¨

- [ê°œìš”](#ê°œìš”)
- [ìŠ¤í† ë¦¬ì§€ ì „ëµ ì´í•´](#ìŠ¤í† ë¦¬ì§€-ì „ëµ-ì´í•´)
- [3.1 ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ ì¤€ë¹„](#31-ì»¨í…Œì´ë„ˆ-ì´ë¯¸ì§€-ì¤€ë¹„)
  - [3.1.1 ECR ë¡œê·¸ì¸](#311-ecr-ë¡œê·¸ì¸)
  - [3.1.2 Docker ì´ë¯¸ì§€ Pull](#312-docker-ì´ë¯¸ì§€-pull)
  - [3.1.3 Enroot SQSH í¬ë§· ë³€í™˜](#313-enroot-sqsh-í¬ë§·-ë³€í™˜)
- [3.2 ë¶„ì‚° í•™ìŠµ ì‘ì—… ì œì¶œ](#32-ë¶„ì‚°-í•™ìŠµ-ì‘ì—…-ì œì¶œ)
  - [3.2.1 Sbatch ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±](#321-sbatch-ìŠ¤í¬ë¦½íŠ¸-ì‘ì„±)
  - [3.2.2 ì‘ì—… ì œì¶œ](#322-ì‘ì—…-ì œì¶œ)
  - [3.2.3 ì‘ì—… ëª¨ë‹ˆí„°ë§](#323-ì‘ì—…-ëª¨ë‹ˆí„°ë§)
- [3.3 í•™ìŠµ ê²°ê³¼ í™•ì¸](#33-í•™ìŠµ-ê²°ê³¼-í™•ì¸)
  - [3.3.1 ë¡œì»¬ ê²°ê³¼ í™•ì¸](#331-ë¡œì»¬-ê²°ê³¼-í™•ì¸)
  - [3.3.2 S3 ë™ê¸°í™” í™•ì¸](#332-s3-ë™ê¸°í™”-í™•ì¸)
- [ë‹¤ìŒ ë‹¨ê³„](#ë‹¤ìŒ-ë‹¨ê³„)

---

## ê°œìš”

ì´ ë¬¸ì„œì—ì„œëŠ” ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:

- âœ… ECRì—ì„œ ì»¤ìŠ¤í…€ DLC ì´ë¯¸ì§€ë¥¼ Enroot SQSH í¬ë§·ìœ¼ë¡œ ë³€í™˜
- âœ… DeepSpeedë¥¼ ì‚¬ìš©í•œ ë¶„ì‚° í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- âœ… Slurm + Pyxisë¡œ ë©€í‹° ë…¸ë“œ í•™ìŠµ ì‹¤í–‰
- âœ… í•™ìŠµ ê²°ê³¼ ë° S3 ë™ê¸°í™” í™•ì¸

---

## 3.1 ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ ì¤€ë¹„

Head Nodeì—ì„œ ECRì˜ ì»¤ìŠ¤í…€ DLC ì´ë¯¸ì§€ë¥¼ Enroot SQSH í¬ë§·ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

> ğŸ’¡ **ì™œ SQSH í¬ë§·ì¸ê°€ìš”?**
> - EnrootëŠ” squashfs ì••ì¶• í¬ë§· ì‚¬ìš©
> - ì½ê¸° ì „ìš©, ê³ ì†, ê³µìœ  ê°€ëŠ¥
> - ì—¬ëŸ¬ ë…¸ë“œê°€ ë™ì‹œì— ì•ˆì „í•˜ê²Œ ì½ì„ ìˆ˜ ìˆìŒ

### 3.1.1 ECR ë¡œê·¸ì¸

Head Nodeì—ì„œ ECRì— ë¡œê·¸ì¸í•©ë‹ˆë‹¤:

```bash
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
source ~/pcluster-env.sh

# ECR ë¡œê·¸ì¸
aws ecr get-login-password --region ${AWS_REGION} | \
  docker login --username AWS --password-stdin ${ECR_REPO_URI}
```

**ì˜ˆìƒ ì¶œë ¥:**
```
WARNING! Your password will be stored unencrypted in /home/ubuntu/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded
```

---

### 3.1.2 Docker ì´ë¯¸ì§€ Pull

HeadNodeì— ì ‘ì†í•´ ECRì—ì„œ ì»¤ìŠ¤í…€ DLC ì´ë¯¸ì§€ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤:
2ë²ˆ ëª¨ë“ˆ(02-pcluster-deployment.md) ì—ì„œ push í–ˆë˜ ECR

```bash
export AWS_REGION=us-east-1
export ECR_REPO_NAME=pytorch-training-custom

# ë¦¬í¬ì§€í† ë¦¬ URI ì €ì¥
export ECR_REPO_URI=$(aws ecr describe-repositories \
  --repository-names ${ECR_REPO_NAME} \
  --region ${AWS_REGION} \
  --query 'repositories[0].repositoryUri' \
  --output text)

export IMAGE_TAG=latest

export TRAINING_IMAGE_URI=${ECR_REPO_URI}:${IMAGE_TAG}

aws ecr get-login-password --region ${AWS_REGION} | docker login --username AWS --password-stdin ${ECR_REPO_URI}

docker pull ${TRAINING_IMAGE_URI}
```

**ì˜ˆìƒ ì¶œë ¥:**
```
Login Succeeded

latest: Pulling from pytorch-training-custom
Digest: sha256:1234567890abcdef...
Status: Downloaded newer image for 123456789012.dkr.ecr.us-east-1.amazonaws.com/pytorch-training-custom:latest
```

> â±ï¸ **ì˜ˆìƒ ì†Œìš” ì‹œê°„:** ì•½ 5-10ë¶„ (ì´ë¯¸ì§€ í¬ê¸° ~15-20GB)

#### ì´ë¯¸ì§€ í™•ì¸

```bash
# ë¡œì»¬ Docker ì´ë¯¸ì§€ í™•ì¸
docker images | grep pytorch-training-custom
```

**ì˜ˆìƒ ì¶œë ¥:**
```
123456789012.dkr.ecr.us-east-1.amazonaws.com/pytorch-training-custom:latest    abc123def456     31.1GB         10.4GB
```

---

### 3.1.3 Enroot SQSH í¬ë§· ë³€í™˜

ë¶€íŠ¸ìŠ¤íŠ¸ë© ìŠ¤í¬ë¦½íŠ¸ë¡œ ìƒì„±ëœ **í—¬í¼ ìŠ¤í¬ë¦½íŠ¸**ë¥¼ ì‚¬ìš©í•˜ì—¬ Docker ì´ë¯¸ì§€ë¥¼ Enroot SQSH í¬ë§·ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

> ğŸ’¡ **ë³€í™˜ í”„ë¡œì„¸ìŠ¤:**
> 1. Docker ì´ë¯¸ì§€ â†’ Enroot import
> 2. `/fsx/containers/images/`ì— `.sqsh` íŒŒì¼ë¡œ ì €ì¥ (ê³µìœ )
> 3. ëª¨ë“  Compute Nodeê°€ ì´ íŒŒì¼ì„ ì½ì–´ì„œ ì‚¬ìš©

#### ë³€í™˜ ì‹¤í–‰

```bash
# import-container.sh í—¬í¼ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©
sudo bash /fsx/import-container.sh ${TRAINING_IMAGE_URI} pytorch-training
```

**ì˜ˆìƒ ì¶œë ¥:**
```
Importing container image...
Source: 123456789012.dkr.ecr.us-east-1.amazonaws.com/pytorch-training-custom:latest
Output: pytorch-training.sqsh
[INFO] Fetching image

d43920b07d21de366b78471c16147bd2a28063ae40c4ef9c9ca02a2a6738146d

[INFO] Extracting image content...
[INFO] Creating squashfs filesystem...

Parallel mksquashfs: Using 32 processors
Creating 4.0 filesystem on /fsx/containers/images/pytorch-training.sqsh, block size 131072.
[===============================================================================================================|] 250989/250989 100%

Exportable Squashfs 4.0 filesystem, lzo compressed, data block size 131072
        uncompressed data, compressed metadata, compressed fragments,
        compressed xattrs, compressed ids
        duplicates are removed
Filesystem size 16766768.27 Kbytes (16373.80 Mbytes)
        90.14% of uncompressed filesystem size (18600340.20 Kbytes)
Inode table size 1663087 bytes (1624.11 Kbytes)
        32.07% of uncompressed inode table size (5185940 bytes)
Directory table size 1567010 bytes (1530.28 Kbytes)
        41.36% of uncompressed directory table size (3788590 bytes)
Number of duplicate files found 11488
Number of inodes 139377
Number of files 116377
Number of fragments 9168
Number of symbolic links 10083
Number of device nodes 0
Number of fifo nodes 0
Number of socket nodes 0
Number of directories 12917
Number of ids (unique uids + gids) 1
Number of uids 1
        root (0)
Number of gids 1
        root (0)

âœ“ Import completed!

Available container images:
-rw-r--r-- 1 root root 16G Nov 29 20:06 /fsx/containers/images/pytorch-training.sqsh
```

> â±ï¸ **ì˜ˆìƒ ì†Œìš” ì‹œê°„:** ì•½ 10-15ë¶„ (ì´ë¯¸ì§€ í¬ê¸°ì™€ ì••ì¶• ì†ë„ì— ë”°ë¼)

#### ë³€í™˜ëœ ì´ë¯¸ì§€ í™•ì¸

```bash
# SQSH ì´ë¯¸ì§€ í™•ì¸
ls -lh /fsx/containers/images/

# Enrootë¡œ ì´ë¯¸ì§€ ëª©ë¡ í™•ì¸
enroot list
```

**ì˜ˆìƒ ì¶œë ¥:**
```
total 17G
-rw-r--r-- 1 root root 16G Nov 29 20:06 pytorch-training.sqsh
```

#### í™˜ê²½ ë³€ìˆ˜ ì €ì¥

ë‚˜ì¤‘ì— ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ í™˜ê²½ ë³€ìˆ˜ë¡œ ì €ì¥í•©ë‹ˆë‹¤:

```bash
export CONTAINER_IMAGE=/fsx/containers/images/pytorch-training.sqsh

echo "Container Image: ${CONTAINER_IMAGE}"
```

---

## 3.2 ë¶„ì‚° í•™ìŠµ ì‘ì—… ì œì¶œ

DeepSpeedë¥¼ ì‚¬ìš©í•œ ë¶„ì‚° í•™ìŠµ ì‘ì—…ì„ Slurmìœ¼ë¡œ ì œì¶œí•©ë‹ˆë‹¤.

### 3.2.1 Sbatch ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

#### í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì´í•´

ë¨¼ì € ì»¨í…Œì´ë„ˆì— í¬í•¨ëœ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì˜ ë™ì‘ì„ ì´í•´í•©ë‹ˆë‹¤:

> ğŸ“ **í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸:** `/workspace/train_distributed_deepspeed.py` (ì»¨í…Œì´ë„ˆ ë‚´ë¶€)
> - ì†ŒìŠ¤ ì½”ë“œ: [examples/containers/pytorch/train_distributed_deepspeed.py](../examples/containers/pytorch/train_distributed_deepspeed.py)

**ì£¼ìš” ê¸°ëŠ¥:**
- Qwen 2.5-0.5B ëª¨ë¸ ì‚¬ìš©
- WikiText-2 ë°ì´í„°ì…‹ ë¡œë“œ (`/lustre/data/wikitext-2/`)
- DeepSpeedë¡œ ë¶„ì‚° í•™ìŠµ
- ì²´í¬í¬ì¸íŠ¸ ìë™ ì €ì¥ (`/lustre/checkpoints/`)
- TensorBoard ë¡œê·¸ (`/lustre/logs/`)
- ìµœì¢… ëª¨ë¸ ì €ì¥ (`/lustre/results/`)

**ìŠ¤í† ë¦¬ì§€ ê²½ë¡œ (í™˜ê²½ ë³€ìˆ˜ë¡œ ì œì–´):**
```python
lustre_base = os.environ.get('LUSTRE_BASE', '/lustre')
experiment_name = os.environ.get('EXPERIMENT_NAME', f'qwen-wikitext-{timestamp}')

data_path = f"{lustre_base}/data/wikitext-2"
checkpoint_path = f"{lustre_base}/checkpoints/{experiment_name}"
log_path = f"{lustre_base}/logs/{experiment_name}"
result_path = f"{lustre_base}/results/{experiment_name}"
```

**S3 ìë™ ë™ê¸°í™”:**
- `/lustre/checkpoints/` â†’ `s3://${S3_BUCKET_NAME}/checkpoints/` (AutoExport)
- `/lustre/logs/` â†’ `s3://${S3_BUCKET_NAME}/logs/` (AutoExport)
- `/lustre/results/` â†’ `s3://${S3_BUCKET_NAME}/results/` (AutoExport)

---

#### Sbatch ìŠ¤í¬ë¦½íŠ¸ ìƒì„±

ë¶„ì‚° í•™ìŠµì„ ìœ„í•œ Slurm ë°°ì¹˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤:

```bash
# Experiment ì´ë¦„ ì„¤ì •
export EXPERIMENT_NAME=qwen-wikitext-$(date +%Y%m%d-%H%M%S)

# Sbatch ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
cat > train-distributed.sbatch << 'EOF'
#!/bin/bash
#SBATCH --job-name=distributed-training
#SBATCH --partition=compute-gpu
#SBATCH --nodes=2                      # 2ê°œ ë…¸ë“œ ì‚¬ìš©
#SBATCH --ntasks-per-node=1            # ë…¸ë“œë‹¹ 1ê°œ íƒœìŠ¤í¬ (DeepSpeed launcher)
#SBATCH --gpus-per-node=1              # ë…¸ë“œë‹¹ 1ê°œ GPU (g5.8xlarge)
#SBATCH --time=02:00:00                # ìµœëŒ€ 2ì‹œê°„
#SBATCH --output=%x-%j.out             #lustreì— ë¡œê·¸ë¥¼ ì €ì¥í• ê±°ë¼ë©´ /lustre/logs/%x-%j.out
#SBATCH --error=%x-%j.err              #lustreì— ë¡œê·¸ë¥¼ ì €ì¥í• ê±°ë¼ë©´ /lustre/logs/%x-%j.err

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export EXPERIMENT_NAME=${EXPERIMENT_NAME:-qwen-wikitext-default}
export LUSTRE_BASE=/lustre
export CONTAINER_IMAGE="/fsx/containers/images/pytorch-training.sqsh"

# Master ë…¸ë“œ ì •ë³´
export MASTER_ADDR=$(scontrol show hostname $SLURM_NODELIST | head -n 1)
export MASTER_PORT=29500

# NCCL ì„¤ì •
export NCCL_DEBUG=INFO
export FI_PROVIDER=efa
export NCCL_PROTO=simple
export NCCL_SOCKET_IFNAME=ens5
export GLOO_SOCKET_IFNAME=ens5
export NCCL_IB_DISABLE=1

# DeepSpeed ì„¤ì •
export DEEPSPEED_CONFIG=/workspace/ds_config.json

echo "=========================================="
echo "Job ID: $SLURM_JOB_ID"
echo "Job Name: $SLURM_JOB_NAME"
echo "Nodes: $SLURM_JOB_NUM_NODES"
echo "Node List: $SLURM_NODELIST"
echo "Master: $MASTER_ADDR:$MASTER_PORT"
echo "GPUs per node: $SLURM_GPUS_PER_NODE"
echo "Experiment: $EXPERIMENT_NAME"
echo "Container: $CONTAINER_IMAGE"
echo "=========================================="

srun --container-image=${CONTAINER_IMAGE} \
     --container-mounts=/dev/infiniband:/dev/infiniband,/lustre:/lustre,/fsx:/fsx \
     --container-writable \
     bash -c "
     torchrun \
       --nproc_per_node=1 \
       --nnodes=${SLURM_JOB_NUM_NODES} \
       --node_rank=\${SLURM_PROCID} \
       --master_addr=${MASTER_ADDR} \
       --master_port=${MASTER_PORT} \
       --rdzv_backend=c10d \
       --rdzv_endpoint=${MASTER_ADDR}:${MASTER_PORT} \
       /workspace/train_distributed_deepspeed.py
     "


echo "=========================================="
echo "Training completed at $(date)"
echo "Results saved to: ${LUSTRE_BASE}/results/${EXPERIMENT_NAME}/"
echo "Checkpoints: ${LUSTRE_BASE}/checkpoints/${EXPERIMENT_NAME}/"
echo "=========================================="

EOF

echo "âœ… Sbatch script created: train-distributed.sbatch"
```

**ìŠ¤í¬ë¦½íŠ¸ ì£¼ìš” êµ¬ì„±:**

| ì„¹ì…˜ | ì„¤ëª… |
|------|------|
| **SBATCH ì§€ì‹œì** | Slurm ë¦¬ì†ŒìŠ¤ ìš”ì²­ (ë…¸ë“œ, GPU, ì‹œê°„ ë“±) |
| **í™˜ê²½ ë³€ìˆ˜** | ì‹¤í—˜ ì´ë¦„, ê²½ë¡œ, NCCL ì„¤ì • |
| **Master ë…¸ë“œ ì„¤ì •** | ë¶„ì‚° í•™ìŠµì„ ìœ„í•œ ë§ˆìŠ¤í„° ì£¼ì†Œ |
| **srun + Pyxis** | ì»¨í…Œì´ë„ˆ ì‹¤í–‰ ë° ë§ˆìš´íŠ¸ |
| **deepspeed launcher** | ë©€í‹° ë…¸ë“œ ë¶„ì‚° í•™ìŠµ ì‹œì‘ |

**ì»¨í…Œì´ë„ˆ ë§ˆìš´íŠ¸ ì„¤ëª…:**
```bash
--container-mounts=/lustre:/lustre,/fsx:/fsx
```
- Hostì˜ `/lustre` â†’ ì»¨í…Œì´ë„ˆì˜ `/lustre` (ë°ì´í„°, ê²°ê³¼)
- Hostì˜ `/fsx` â†’ ì»¨í…Œì´ë„ˆì˜ `/fsx` (ì½”ë“œ, ì„¤ì •)

**DeepSpeed íŒŒë¼ë¯¸í„°:**
```bash
--num_nodes=${SLURM_JOB_NUM_NODES}     # Slurmì´ í• ë‹¹í•œ ë…¸ë“œ ìˆ˜
--num_gpus=${SLURM_GPUS_PER_NODE}      # ë…¸ë“œë‹¹ GPU ìˆ˜
--master_addr=${MASTER_ADDR}           # ì²« ë²ˆì§¸ ë…¸ë“œ ì£¼ì†Œ
--master_port=${MASTER_PORT}           # í†µì‹  í¬íŠ¸
--node_rank=${SLURM_NODEID}            # í˜„ì¬ ë…¸ë“œ ìˆœë²ˆ
```

---

### 3.2.2 ì‘ì—… ì œì¶œ

Sbatch ìŠ¤í¬ë¦½íŠ¸ë¥¼ Slurmì— ì œì¶œí•©ë‹ˆë‹¤:

```bash
# ì‘ì—… ì œì¶œ
sbatch train-distributed.sbatch
```

**ì˜ˆìƒ ì¶œë ¥:**
```
Submitted batch job 2

```

---
### 3.2.3 ì‘ì—… ëª¨ë‹ˆí„°ë§

#### ì‘ì—… í í™•ì¸

```bash
# ì‘ì—… ìƒíƒœ í™•ì¸
squeue

scontrol show job ${JOB_ID}
```

#### ì‹¤ì‹œê°„ ë¡œê·¸ í™•ì¸

```bash
# í‘œì¤€ ì¶œë ¥ ë¡œê·¸ (ì‹¤ì‹œê°„)
tail -f distributed-training-${JOB_ID}.out