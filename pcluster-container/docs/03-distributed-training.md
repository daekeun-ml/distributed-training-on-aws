# 3. ë¶„ì‚° í•™ìŠµ ì‹¤í–‰

> ğŸ’¡ **ëª©í‘œ:** ì»¨í…Œì´ë„ˆ ê¸°ë°˜ ë¶„ì‚° í•™ìŠµ ì‘ì—…ì„ Slurmìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.

â±ï¸ **ì˜ˆìƒ ì†Œìš” ì‹œê°„:** 40-60ë¶„

## ëª©ì°¨

- [ê°œìš”](#ê°œìš”)
- [ìŠ¤í† ë¦¬ì§€ ì „ëµ ì´í•´](#ìŠ¤í† ë¦¬ì§€-ì „ëµ-ì´í•´)
- [3.1 ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ ì¤€ë¹„](#31-ì»¨í…Œì´ë„ˆ-ì´ë¯¸ì§€-ì¤€ë¹„)
  - [3.1.1 ECR ë¡œê·¸ì¸](#311-ecr-ë¡œê·¸ì¸)
  - [3.1.2 Docker ì´ë¯¸ì§€ Pull](#312-docker-ì´ë¯¸ì§€-pull)
  - [3.1.3 Enroot SQSH í¬ë§· ë³€í™˜](#313-enroot-sqsh-í¬ë§·-ë³€í™˜)
- [3.2 ë¶„ì‚° í•™ìŠµ ì‘ì—… ì œì¶œ](#32-ë¶„ì‚°-í•™ìŠµ-ì‘ì—…-ì œì¶œ)
  - [3.2.1 Sbatch ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±](#321-sbatch-ìŠ¤í¬ë¦½íŠ¸-ì‘ì„±)
  - [3.2.2 ì‘ì—… ì œì¶œ](#322-ì‘ì—…-ì œì¶œ)
  - [3.2.3 ì‘ì—… ëª¨ë‹ˆí„°ë§](#323-ì‘ì—…-ëª¨ë‹ˆí„°ë§)
- [3.3 í•™ìŠµ ê²°ê³¼ í™•ì¸](#33-í•™ìŠµ-ê²°ê³¼-í™•ì¸)
  - [3.3.1 ë¡œì»¬ ê²°ê³¼ í™•ì¸](#331-ë¡œì»¬-ê²°ê³¼-í™•ì¸)
  - [3.3.2 S3 ë™ê¸°í™” í™•ì¸](#332-s3-ë™ê¸°í™”-í™•ì¸)
- [ë‹¤ìŒ ë‹¨ê³„](#ë‹¤ìŒ-ë‹¨ê³„)

---

## ê°œìš”

ì´ ë¬¸ì„œì—ì„œëŠ” ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:

- âœ… ECRì—ì„œ ì»¤ìŠ¤í…€ DLC ì´ë¯¸ì§€ë¥¼ Enroot SQSH í¬ë§·ìœ¼ë¡œ ë³€í™˜
- âœ… DeepSpeedë¥¼ ì‚¬ìš©í•œ ë¶„ì‚° í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- âœ… Slurm + Pyxisë¡œ ë©€í‹° ë…¸ë“œ í•™ìŠµ ì‹¤í–‰
- âœ… í•™ìŠµ ê²°ê³¼ ë° S3 ë™ê¸°í™” í™•ì¸

---

## 3.1 ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ ì¤€ë¹„

Head Nodeì—ì„œ ECRì˜ ì»¤ìŠ¤í…€ DLC ì´ë¯¸ì§€ë¥¼ Enroot SQSH í¬ë§·ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

> ğŸ’¡ **ì™œ SQSH í¬ë§·ì¸ê°€ìš”?**
> - EnrootëŠ” squashfs ì••ì¶• í¬ë§· ì‚¬ìš©
> - ì½ê¸° ì „ìš©, ê³ ì†, ê³µìœ  ê°€ëŠ¥
> - ì—¬ëŸ¬ ë…¸ë“œê°€ ë™ì‹œì— ì•ˆì „í•˜ê²Œ ì½ì„ ìˆ˜ ìˆìŒ

### 3.1.1 ECR ë¡œê·¸ì¸

Head Nodeì—ì„œ ECRì— ë¡œê·¸ì¸í•©ë‹ˆë‹¤:

```bash
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
source ~/pcluster-env.sh

# ECR ë¡œê·¸ì¸
aws ecr get-login-password --region ${AWS_REGION} | \
  docker login --username AWS --password-stdin ${ECR_REPO_URI}
```

**ì˜ˆìƒ ì¶œë ¥:**
```
WARNING! Your password will be stored unencrypted in /home/ubuntu/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded
```

---

### 3.1.2 Docker ì´ë¯¸ì§€ Pull

HeadNodeì— ì ‘ì†í•´ ECRì—ì„œ ì»¤ìŠ¤í…€ DLC ì´ë¯¸ì§€ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤:
2ë²ˆ ëª¨ë“ˆ(02-pcluster-deployment.md) ì—ì„œ push í–ˆë˜ ECR

```bash
export AWS_REGION=us-east-1
export ECR_REPO_NAME=pytorch-training-custom

# ë¦¬í¬ì§€í† ë¦¬ URI ì €ì¥
export ECR_REPO_URI=$(aws ecr describe-repositories \
  --repository-names ${ECR_REPO_NAME} \
  --region ${AWS_REGION} \
  --query 'repositories[0].repositoryUri' \
  --output text)

export IMAGE_TAG=latest

export TRAINING_IMAGE_URI=${ECR_REPO_URI}:${IMAGE_TAG}

aws ecr get-login-password --region ${AWS_REGION} | docker login --username AWS --password-stdin ${ECR_REPO_URI}

docker pull ${TRAINING_IMAGE_URI}
```

**ì˜ˆìƒ ì¶œë ¥:**
```
Login Succeeded

latest: Pulling from pytorch-training-custom
Digest: sha256:1234567890abcdef...
Status: Downloaded newer image for 123456789012.dkr.ecr.us-east-1.amazonaws.com/pytorch-training-custom:latest
```

> â±ï¸ **ì˜ˆìƒ ì†Œìš” ì‹œê°„:** ì•½ 5-10ë¶„ (ì´ë¯¸ì§€ í¬ê¸° ~15-20GB)

#### ì´ë¯¸ì§€ í™•ì¸

```bash
# ë¡œì»¬ Docker ì´ë¯¸ì§€ í™•ì¸
docker images | grep pytorch-training-custom
```

**ì˜ˆìƒ ì¶œë ¥:**
```
123456789012.dkr.ecr.us-east-1.amazonaws.com/pytorch-training-custom:latest    abc123def456     31.1GB         10.4GB
```

---

### 3.1.3 Enroot SQSH í¬ë§· ë³€í™˜

ë¶€íŠ¸ìŠ¤íŠ¸ë© ìŠ¤í¬ë¦½íŠ¸ë¡œ ìƒì„±ëœ **í—¬í¼ ìŠ¤í¬ë¦½íŠ¸**ë¥¼ ì‚¬ìš©í•˜ì—¬ Docker ì´ë¯¸ì§€ë¥¼ Enroot SQSH í¬ë§·ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

> ğŸ’¡ **ë³€í™˜ í”„ë¡œì„¸ìŠ¤:**
> 1. Docker ì´ë¯¸ì§€ â†’ Enroot import
> 2. `/fsx/containers/images/`ì— `.sqsh` íŒŒì¼ë¡œ ì €ì¥ (ê³µìœ )
> 3. ëª¨ë“  Compute Nodeê°€ ì´ íŒŒì¼ì„ ì½ì–´ì„œ ì‚¬ìš©

#### ë³€í™˜ ì‹¤í–‰

```bash
# import-container.sh í—¬í¼ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©
sudo bash /fsx/import-container.sh ${TRAINING_IMAGE_URI} pytorch-training
```

**ì˜ˆìƒ ì¶œë ¥:**
```
Importing container image...
Source: 123456789012.dkr.ecr.us-east-1.amazonaws.com/pytorch-training-custom:latest
Output: pytorch-training.sqsh
[INFO] Fetching image

d43920b07d21de366b78471c16147bd2a28063ae40c4ef9c9ca02a2a6738146d

[INFO] Extracting image content...
[INFO] Creating squashfs filesystem...

Parallel mksquashfs: Using 32 processors
Creating 4.0 filesystem on /fsx/containers/images/pytorch-training.sqsh, block size 131072.
[===============================================================================================================|] 250989/250989 100%

Exportable Squashfs 4.0 filesystem, lzo compressed, data block size 131072
        uncompressed data, compressed metadata, compressed fragments,
        compressed xattrs, compressed ids
        duplicates are removed
Filesystem size 16766768.27 Kbytes (16373.80 Mbytes)
        90.14% of uncompressed filesystem size (18600340.20 Kbytes)
Inode table size 1663087 bytes (1624.11 Kbytes)
        32.07% of uncompressed inode table size (5185940 bytes)
Directory table size 1567010 bytes (1530.28 Kbytes)
        41.36% of uncompressed directory table size (3788590 bytes)
Number of duplicate files found 11488
Number of inodes 139377
Number of files 116377
Number of fragments 9168
Number of symbolic links 10083
Number of device nodes 0
Number of fifo nodes 0
Number of socket nodes 0
Number of directories 12917
Number of ids (unique uids + gids) 1
Number of uids 1
        root (0)
Number of gids 1
        root (0)

âœ“ Import completed!

Available container images:
-rw-r--r-- 1 root root 16G Nov 29 20:06 /fsx/containers/images/pytorch-training.sqsh
```

> â±ï¸ **ì˜ˆìƒ ì†Œìš” ì‹œê°„:** ì•½ 10-15ë¶„ (ì´ë¯¸ì§€ í¬ê¸°ì™€ ì••ì¶• ì†ë„ì— ë”°ë¼)

#### ë³€í™˜ëœ ì´ë¯¸ì§€ í™•ì¸

```bash
# SQSH ì´ë¯¸ì§€ í™•ì¸
ls -lh /fsx/containers/images/

# Enrootë¡œ ì´ë¯¸ì§€ ëª©ë¡ í™•ì¸
enroot list
```

**ì˜ˆìƒ ì¶œë ¥:**
```
total 17G
-rw-r--r-- 1 root root 16G Nov 29 20:06 pytorch-training.sqsh
```

#### í™˜ê²½ ë³€ìˆ˜ ì €ì¥

ë‚˜ì¤‘ì— ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ í™˜ê²½ ë³€ìˆ˜ë¡œ ì €ì¥í•©ë‹ˆë‹¤:

```bash
export CONTAINER_IMAGE=/fsx/containers/images/pytorch-training.sqsh

echo "Container Image: ${CONTAINER_IMAGE}"
```

---

## 3.2 ë¶„ì‚° í•™ìŠµ ì‘ì—… ì œì¶œ

DeepSpeedë¥¼ ì‚¬ìš©í•œ ë¶„ì‚° í•™ìŠµ ì‘ì—…ì„ Slurmìœ¼ë¡œ ì œì¶œí•©ë‹ˆë‹¤.

### 3.2.1 Sbatch ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

#### í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì´í•´

ë¨¼ì € ì»¨í…Œì´ë„ˆì— í¬í•¨ëœ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì˜ ë™ì‘ì„ ì´í•´í•©ë‹ˆë‹¤:

> ğŸ“ **í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸:** `/workspace/train_distributed_deepspeed.py` (ì»¨í…Œì´ë„ˆ ë‚´ë¶€)
> - ì†ŒìŠ¤ ì½”ë“œ: [examples/containers/pytorch/train_distributed_deepspeed.py](../examples/containers/pytorch/train_distributed_deepspeed.py)

**ì£¼ìš” ê¸°ëŠ¥:**
- Qwen 2.5-0.5B ëª¨ë¸ ì‚¬ìš©
- WikiText-2 ë°ì´í„°ì…‹ ë¡œë“œ (`/lustre/data/wikitext-2/`)
- DeepSpeedë¡œ ë¶„ì‚° í•™ìŠµ
- ì²´í¬í¬ì¸íŠ¸ ìë™ ì €ì¥ (`/lustre/checkpoints/`)
- TensorBoard ë¡œê·¸ (`/lustre/logs/`)
- ìµœì¢… ëª¨ë¸ ì €ì¥ (`/lustre/results/`)

**ìŠ¤í† ë¦¬ì§€ ê²½ë¡œ (í™˜ê²½ ë³€ìˆ˜ë¡œ ì œì–´):**
```python
lustre_base = os.environ.get('LUSTRE_BASE', '/lustre')
experiment_name = os.environ.get('EXPERIMENT_NAME', f'qwen-wikitext-{timestamp}')

data_path = f"{lustre_base}/data/wikitext-2"
checkpoint_path = f"{lustre_base}/checkpoints/{experiment_name}"
log_path = f"{lustre_base}/logs/{experiment_name}"
result_path = f"{lustre_base}/results/{experiment_name}"
```

**S3 ìë™ ë™ê¸°í™”:**
- `/lustre/checkpoints/` â†’ `s3://${S3_BUCKET_NAME}/checkpoints/` (AutoExport)
- `/lustre/logs/` â†’ `s3://${S3_BUCKET_NAME}/logs/` (AutoExport)
- `/lustre/results/` â†’ `s3://${S3_BUCKET_NAME}/results/` (AutoExport)

---

#### Sbatch ìŠ¤í¬ë¦½íŠ¸ ìƒì„±

ë¶„ì‚° í•™ìŠµì„ ìœ„í•œ Slurm ë°°ì¹˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤:

```bash
# Experiment ì´ë¦„ ì„¤ì •
export EXPERIMENT_NAME=qwen-wikitext-$(date +%Y%m%d-%H%M%S)

# Sbatch ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
cat > train-distributed.sbatch << 'EOF'
#!/bin/bash
#SBATCH --job-name=distributed-training
#SBATCH --partition=compute-gpu
#SBATCH --nodes=2                      # 2ê°œ ë…¸ë“œ ì‚¬ìš©
#SBATCH --ntasks-per-node=1            # ë…¸ë“œë‹¹ 1ê°œ íƒœìŠ¤í¬ (DeepSpeed launcher)
#SBATCH --gpus-per-node=1              # ë…¸ë“œë‹¹ 1ê°œ GPU (g5.8xlarge)
#SBATCH --time=02:00:00                # ìµœëŒ€ 2ì‹œê°„
#SBATCH --output=%x-%j.out             #lustreì— ë¡œê·¸ë¥¼ ì €ì¥í• ê±°ë¼ë©´ /lustre/logs/%x-%j.out
#SBATCH --error=%x-%j.err              #lustreì— ë¡œê·¸ë¥¼ ì €ì¥í• ê±°ë¼ë©´ /lustre/logs/%x-%j.err

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export EXPERIMENT_NAME=${EXPERIMENT_NAME:-qwen-wikitext-default}
export LUSTRE_BASE=/lustre
export CONTAINER_IMAGE="/fsx/containers/images/pytorch-training.sqsh"

# Master ë…¸ë“œ ì •ë³´
export MASTER_ADDR=$(scontrol show hostname $SLURM_NODELIST | head -n 1)
export MASTER_PORT=29500

# NCCL ì„¤ì •
export NCCL_DEBUG=INFO
export FI_PROVIDER=efa
export NCCL_PROTO=simple
export NCCL_SOCKET_IFNAME=ens5
export GLOO_SOCKET_IFNAME=ens5
export NCCL_IB_DISABLE=1

# DeepSpeed ì„¤ì •
export DEEPSPEED_CONFIG=/workspace/ds_config.json

echo "=========================================="
echo "Job ID: $SLURM_JOB_ID"
echo "Job Name: $SLURM_JOB_NAME"
echo "Nodes: $SLURM_JOB_NUM_NODES"
echo "Node List: $SLURM_NODELIST"
echo "Master: $MASTER_ADDR:$MASTER_PORT"
echo "GPUs per node: $SLURM_GPUS_PER_NODE"
echo "Experiment: $EXPERIMENT_NAME"
echo "Container: $CONTAINER_IMAGE"
echo "=========================================="

srun --container-image=${CONTAINER_IMAGE} \
     --container-mounts=/dev/infiniband:/dev/infiniband,/lustre:/lustre,/fsx:/fsx \
     --container-writable \
     bash -c "
     torchrun \
       --nproc_per_node=1 \
       --nnodes=${SLURM_JOB_NUM_NODES} \
       --node_rank=\${SLURM_PROCID} \
       --master_addr=${MASTER_ADDR} \
       --master_port=${MASTER_PORT} \
       --rdzv_backend=c10d \
       --rdzv_endpoint=${MASTER_ADDR}:${MASTER_PORT} \
       /workspace/train_distributed_deepspeed.py
     "


echo "=========================================="
echo "Training completed at $(date)"
echo "Results saved to: ${LUSTRE_BASE}/results/${EXPERIMENT_NAME}/"
echo "Checkpoints: ${LUSTRE_BASE}/checkpoints/${EXPERIMENT_NAME}/"
echo "=========================================="

EOF

echo "âœ… Sbatch script created: train-distributed.sbatch"
```

**ìŠ¤í¬ë¦½íŠ¸ ì£¼ìš” êµ¬ì„±:**

| ì„¹ì…˜ | ì„¤ëª… |
|------|------|
| **SBATCH ì§€ì‹œì** | Slurm ë¦¬ì†ŒìŠ¤ ìš”ì²­ (ë…¸ë“œ, GPU, ì‹œê°„ ë“±) |
| **í™˜ê²½ ë³€ìˆ˜** | ì‹¤í—˜ ì´ë¦„, ê²½ë¡œ, NCCL ì„¤ì • |
| **Master ë…¸ë“œ ì„¤ì •** | ë¶„ì‚° í•™ìŠµì„ ìœ„í•œ ë§ˆìŠ¤í„° ì£¼ì†Œ |
| **srun + Pyxis** | ì»¨í…Œì´ë„ˆ ì‹¤í–‰ ë° ë§ˆìš´íŠ¸ |
| **deepspeed launcher** | ë©€í‹° ë…¸ë“œ ë¶„ì‚° í•™ìŠµ ì‹œì‘ |

**ì»¨í…Œì´ë„ˆ ë§ˆìš´íŠ¸ ì„¤ëª…:**
```bash
--container-mounts=/lustre:/lustre,/fsx:/fsx
```
- Hostì˜ `/lustre` â†’ ì»¨í…Œì´ë„ˆì˜ `/lustre` (ë°ì´í„°, ê²°ê³¼)
- Hostì˜ `/fsx` â†’ ì»¨í…Œì´ë„ˆì˜ `/fsx` (ì½”ë“œ, ì„¤ì •)

**DeepSpeed íŒŒë¼ë¯¸í„°:**
```bash
--num_nodes=${SLURM_JOB_NUM_NODES}     # Slurmì´ í• ë‹¹í•œ ë…¸ë“œ ìˆ˜
--num_gpus=${SLURM_GPUS_PER_NODE}      # ë…¸ë“œë‹¹ GPU ìˆ˜
--master_addr=${MASTER_ADDR}           # ì²« ë²ˆì§¸ ë…¸ë“œ ì£¼ì†Œ
--master_port=${MASTER_PORT}           # í†µì‹  í¬íŠ¸
--node_rank=${SLURM_NODEID}            # í˜„ì¬ ë…¸ë“œ ìˆœë²ˆ
```

---

### 3.2.2 ì‘ì—… ì œì¶œ

Sbatch ìŠ¤í¬ë¦½íŠ¸ë¥¼ Slurmì— ì œì¶œí•©ë‹ˆë‹¤:

```bash
# ì‘ì—… ì œì¶œ
sbatch train-distributed.sbatch
```

**ì˜ˆìƒ ì¶œë ¥:**
```
Submitted batch job 2
```

---
### 3.2.3 ì‘ì—… ëª¨ë‹ˆí„°ë§

#### ì‘ì—… í í™•ì¸

```bash
# ì‘ì—… ìƒíƒœ í™•ì¸
export JOB_ID=$(squeue -u $USER -h -o %i | head -n 1)
[ -z "$JOB_ID" ] && read -p "Enter JOB_ID: " JOB_ID
echo "JOB_ID: $JOB_ID"

squeue

scontrol show job ${JOB_ID}
```

#### ì‹¤ì‹œê°„ ë¡œê·¸ í™•ì¸

```bash
# í‘œì¤€ ì¶œë ¥ ë¡œê·¸ (ì‹¤ì‹œê°„)
tail -f distributed-training-${JOB_ID}.out
```

**ì˜ˆìƒ ë¡œê·¸ ì¶œë ¥:**
```
$ tail -f distributed-training-12.out
Job ID: 12
Job Name: distributed-training
Nodes: 2
Node List: compute-gpu-st-distributed-ml-[1-2]
Master: compute-gpu-st-distributed-ml-1:29500
GPUs per node: 1
Experiment: qwen-wikitext-20251129-201659
Container: /fsx/containers/images/pytorch-training.sqsh
==========================================
directory ì„ì‹œ ì¡°ì¹˜

âš  /scratch not found, using /tmp for caching
ğŸ“¦ Cache directories:
   HF_DATASETS_CACHE: /tmp/hf_cache_1000
   TRANSFORMERS_CACHE: /tmp/transformers_cache_1000
   TRITON_CACHE_DIR: /tmp/triton_cache_1000

======================================================================
ğŸš€ Distributed Training Starting
======================================================================
ğŸ“ Node: compute-gpu-st-distributed-ml-1
ğŸ¯ Rank: 0/2 (Local Rank: 0)
ğŸ“‚ Dataset: /lustre/data/wikitext-2
ğŸ’¾ Checkpoints: /lustre/checkpoints/qwen-wikitext-20251129-201659
ğŸ“Š Logs: /lustre/logs/qwen-wikitext-20251129-201659
ğŸ¯ Results: /lustre/results/qwen-wikitext-20251129-201659
ğŸ”§ Experiment: qwen-wikitext-20251129-201659
======================================================================

[Rank 0] Creating output directories...
âœ“ [Rank 0] Output directories created and writable
âš  /scratch not found, using /tmp for caching
ğŸ“¦ Cache directories:
   HF_DATASETS_CACHE: /tmp/hf_cache_1000
   TRANSFORMERS_CACHE: /tmp/transformers_cache_1000
   TRITON_CACHE_DIR: /tmp/triton_cache_1000

======================================================================
ğŸš€ Distributed Training Starting
======================================================================
ğŸ“ Node: compute-gpu-st-distributed-ml-2
ğŸ¯ Rank: 1/2 (Local Rank: 0)
ğŸ“‚ Dataset: /lustre/data/wikitext-2
ğŸ’¾ Checkpoints: /lustre/checkpoints/qwen-wikitext-20251129-201659
ğŸ“Š Logs: /lustre/logs/qwen-wikitext-20251129-201659
ğŸ¯ Results: /lustre/results/qwen-wikitext-20251129-201659
ğŸ”§ Experiment: qwen-wikitext-20251129-201659
======================================================================

compute-gpu-st-distributed-ml-1:7614:7614 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ens5
compute-gpu-st-distributed-ml-1:7614:7614 [0] NCCL INFO Bootstrap : Using ens5:10.1.94.228<0>
compute-gpu-st-distributed-ml-1:7614:7614 [0] NCCL INFO cudaDriverVersion 12080
compute-gpu-st-distributed-ml-1:7614:7614 [0] NCCL INFO NCCL version 2.23.4+cuda12.4
compute-gpu-st-distributed-ml-2:7070:7070 [0] NCCL INFO cudaDriverVersion 12080
compute-gpu-st-distributed-ml-2:7070:7070 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ens5
compute-gpu-st-distributed-ml-2:7070:7070 [0] NCCL INFO Bootstrap : Using ens5:10.1.113.246<0>
compute-gpu-st-distributed-ml-2:7070:7070 [0] NCCL INFO NCCL version 2.23.4+cuda12.4
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.12.1-aws
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO NET/OFI Using CUDA driver version 12080
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO NET/OFI Internode latency set at 150.0 us
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO NET/OFI Creating one domain per process
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO NET/OFI Global registrations supported
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so.
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO Using network AWS Libfabric
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO DMA-BUF is available on GPU device 0
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO ncclCommInitRankConfig comm 0x573f9cb382e0 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1e0 commId 0x1ce724326e09a7e5 - Init START
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.12.1-aws
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO NET/OFI Using CUDA driver version 12080
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO NET/OFI Configuring AWS-specific options
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO NET/OFI Internode latency set at 150.0 us
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO NET/OFI Creating one domain per process
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO NET/OFI Selected Provider is efa (found 1 nics)
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO NET/OFI Global registrations supported
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so.
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO Using network AWS Libfabric
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO DMA-BUF is available on GPU device 0
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO ncclCommInitRankConfig comm 0x60630f7f62f0 rank 1 nranks 2 cudaDev 0 nvmlDev 0 busId 1e0 commId 0x1ce724326e09a7e5 - Init START
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO Bootstrap timings total 0.040572 (create 0.000033, send 0.000142, recv 0.039682, ring 0.000075, delay 0.000000)
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO Bootstrap timings total 0.001249 (create 0.000031, send 0.000262, recv 0.000439, ring 0.000280, delay 0.000000)
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO NET/OFI Global registrations supported
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO NET/OFI Global registrations supported
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO comm 0x573f9cb382e0 rank 0 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO Channel 00/02 : 0 1
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO Channel 01/02 : 0 1
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO P2P Chunksize set to 131072
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO comm 0x60630f7f62f0 rank 1 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO P2P Chunksize set to 131072
compute-gpu-st-distributed-ml-1:7614:7692 [0] NCCL INFO [Proxy Service] Device 0 CPU core 9
compute-gpu-st-distributed-ml-2:7070:7147 [0] NCCL INFO [Proxy Service] Device 0 CPU core 22
compute-gpu-st-distributed-ml-2:7070:7148 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 11
compute-gpu-st-distributed-ml-1:7614:7693 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 12
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO NCCL_PROTO set by environment to simple
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO CC Off, Multi-GPU CC Off, workFifoBytes 1048576
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO ncclCommInitRankConfig comm 0x573f9cb382e0 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1e0 commId 0x1ce724326e09a7e5 - Init COMPLETE
compute-gpu-st-distributed-ml-1:7614:7691 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 2 total 0.21 (kernels 0.14, alloc 0.01, bootstrap 0.04, allgathers 0.00, topo 0.00, graphs 0.00, connections 0.00, rest 0.00)
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO ncclCommInitRankConfig comm 0x60630f7f62f0 rank 1 nranks 2 cudaDev 0 nvmlDev 0 busId 1e0 commId 0x1ce724326e09a7e5 - Init COMPLETE
compute-gpu-st-distributed-ml-2:7070:7146 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 2 total 0.17 (kernels 0.14, alloc 0.01, bootstrap 0.00, allgathers 0.00, topo 0.00, graphs 0.00, connections 0.00, rest 0.00)
compute-gpu-st-distributed-ml-1:7614:7695 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 11
compute-gpu-st-distributed-ml-1:7614:7692 [0] NCCL INFO NET/OFI Global registrations supported
compute-gpu-st-distributed-ml-2:7070:7150 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 7
compute-gpu-st-distributed-ml-2:7070:7147 [0] NCCL INFO NET/OFI Global registrations supported
compute-gpu-st-distributed-ml-1:7614:7694 [0] NCCL INFO Channel 00/0 : 1[0] -> 0[0] [receive] via NET/AWS Libfabric/0
compute-gpu-st-distributed-ml-1:7614:7692 [0] NCCL INFO NET/OFI Global registrations supported
compute-gpu-st-distributed-ml-1:7614:7694 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[0] [receive] via NET/AWS Libfabric/0
compute-gpu-st-distributed-ml-1:7614:7692 [0] NCCL INFO NET/OFI Global registrations supported
compute-gpu-st-distributed-ml-1:7614:7694 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/AWS Libfabric/0
compute-gpu-st-distributed-ml-1:7614:7692 [0] NCCL INFO NET/OFI Global registrations supported
compute-gpu-st-distributed-ml-1:7614:7694 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/AWS Libfabric/0
compute-gpu-st-distributed-ml-2:7070:7149 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [receive] via NET/AWS Libfabric/0
compute-gpu-st-distributed-ml-2:7070:7147 [0] NCCL INFO NET/OFI Global registrations supported
compute-gpu-st-distributed-ml-2:7070:7149 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [receive] via NET/AWS Libfabric/0
compute-gpu-st-distributed-ml-2:7070:7147 [0] NCCL INFO NET/OFI Global registrations supported
compute-gpu-st-distributed-ml-2:7070:7149 [0] NCCL INFO Channel 00/0 : 1[0] -> 0[0] [send] via NET/AWS Libfabric/0
compute-gpu-st-distributed-ml-2:7070:7147 [0] NCCL INFO NET/OFI Global registrations supported
compute-gpu-st-distributed-ml-2:7070:7149 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[0] [send] via NET/AWS Libfabric/0
compute-gpu-st-distributed-ml-1:7614:7694 [0] NCCL INFO Connected all rings
compute-gpu-st-distributed-ml-2:7070:7149 [0] NCCL INFO Connected all rings
[Rank 0] Synchronized, proceeding...
[Rank 0] Loading Qwen2.5-0.5B model...
[Rank 1] Synchronized, proceeding...
[Rank 1] Loading Qwen2.5-0.5B model...
[Rank 1] Loading tokenizer...
[Rank 0] Loading tokenizer...
[Rank 1] Loading WikiText-2 dataset from /lustre/data/wikitext-2...
[Rank 1] Original dataset size: 36718
[Rank 1] Filtering empty texts...
[Rank 0] Loading WikiText-2 dataset from /lustre/data/wikitext-2...
[Rank 0] Original dataset size: 36718
[Rank 0] Filtering empty texts...
[Rank 1] Filtered dataset size: 23767
[Rank 1] Tokenizing dataset...
[Rank 0] Filtered dataset size: 23767
[Rank 0] Tokenizing dataset...
[Rank 1] Tokenization complete. Dataset size: 23767
[Rank 1] Setting up training configuration...
[Rank 0] Tokenization complete. Dataset size: 23767
[Rank 0] Setting up training configuration...

======================================================================
ğŸ‹ï¸  [Rank 1] Starting Distributed Training
======================================================================
ğŸ“Š Total Samples: 23767
ğŸ”¢ Batch Size per Device: 4
ğŸ”„ Gradient Accumulation Steps: 2
ğŸŒ World Size: 2
ğŸ“ˆ Effective Batch Size: 16
ğŸ“ Logs: tensorboard --logdir=/lustre/logs/qwen-wikitext-20251129-201659
======================================================================


======================================================================
ğŸ‹ï¸  [Rank 0] Starting Distributed Training
======================================================================
ğŸ“Š Total Samples: 23767
ğŸ”¢ Batch Size per Device: 4
ğŸ”„ Gradient Accumulation Steps: 2
ğŸŒ World Size: 2
ğŸ“ˆ Effective Batch Size: 16
ğŸ“ Logs: tensorboard --logdir=/lustre/logs/qwen-wikitext-20251129-201659
======================================================================

compute-gpu-st-distributed-ml-2:7070:7351 [0] NCCL INFO Connected binomial trees
compute-gpu-st-distributed-ml-1:7614:7888 [0] NCCL INFO Connected binomial trees
compute-gpu-st-distributed-ml-1:7614:8102 [0] NCCL INFO Connected all trees
compute-gpu-st-distributed-ml-2:7070:7561 [0] NCCL INFO Connected all trees
{'loss': 7.4095, 'grad_norm': 3120.0, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 6.3676, 'grad_norm': 2112.0, 'learning_rate': 1.8000000000000001e-06, 'epoch': 0.01}
{'loss': 1.7161, 'grad_norm': 18.0, 'learning_rate': 3.8000000000000005e-06, 'epoch': 0.01}
{'loss': 0.637, 'grad_norm': 6.4375, 'learning_rate': 5.8e-06, 'epoch': 0.02}
{'loss': 0.6704, 'grad_norm': 4.8125, 'learning_rate': 7.800000000000002e-06, 'epoch': 0.03}
{'loss': 0.7214, 'grad_norm': 4.5625, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.03}
{'loss': 0.5843, 'grad_norm': 3.71875, 'learning_rate': 1.18e-05, 'epoch': 0.04}
{'loss': 0.7451, 'grad_norm': 3.25, 'learning_rate': 1.38e-05, 'epoch': 0.05}
{'loss': 0.5315, 'grad_norm': 3.484375, 'learning_rate': 1.58e-05, 'epoch': 0.05}
{'loss': 0.5753, 'grad_norm': 2.765625, 'learning_rate': 1.7800000000000002e-05, 'epoch': 0.06}
{'loss': 0.5538, 'grad_norm': 5.5, 'learning_rate': 1.98e-05, 'epoch': 0.07}
{'loss': 0.553, 'grad_norm': 3.1875, 'learning_rate': 1.993732590529248e-05, 'epoch': 0.07}
{'loss': 0.6442, 'grad_norm': 2.859375, 'learning_rate': 1.9867688022284122e-05, 'epoch': 0.08}
{'loss': 0.5673, 'grad_norm': 2.8125, 'learning_rate': 1.979805013927577e-05, 'epoch': 0.09}
{'loss': 0.5269, 'grad_norm': 3.125, 'learning_rate': 1.972841225626741e-05, 'epoch': 0.09}

...

{'loss': 0.5743, 'grad_norm': 2.59375, 'learning_rate': 1.0396935933147634e-05, 'epoch': 1.0}
{'loss': 0.5684, 'grad_norm': 3.09375, 'learning_rate': 1.0327298050139276e-05, 'epoch': 1.0}
{'loss': 0.5329, 'grad_norm': 2.46875, 'learning_rate': 1.0257660167130921e-05, 'epoch': 1.01}
{'loss': 0.4766, 'grad_norm': 2.53125, 'learning_rate': 1.0188022284122564e-05, 'epoch': 1.02}
{'loss': 0.5193, 'grad_norm': 3.328125, 'learning_rate': 1.0118384401114208e-05, 'epoch': 1.02}
{'loss': 0.5086, 'grad_norm': 2.703125, 'learning_rate': 1.0048746518105849e-05, 'epoch': 1.03}
{'loss': 0.5069, 'grad_norm': 3.1875, 'learning_rate': 9.979108635097493e-06, 'epoch': 1.04}
{'loss': 0.4974, 'grad_norm': 3.03125, 'learning_rate': 9.909470752089138e-06, 'epoch': 1.04}
{'loss': 0.5145, 'grad_norm': 2.703125, 'learning_rate': 9.83983286908078e-06, 'epoch': 1.05}

...

{'loss': 0.5762, 'grad_norm': 2.765625, 'learning_rate': 1.601671309192201e-07, 'epoch': 1.99}
{'loss': 0.4222, 'grad_norm': 2.46875, 'learning_rate': 9.052924791086352e-08, 'epoch': 1.99}
{'loss': 0.5291, 'grad_norm': 2.65625, 'learning_rate': 2.0891364902506967e-08, 'epoch': 2.0}
{'train_runtime': 2233.4294, 'train_samples_per_second': 21.283, 'train_steps_per_second': 1.331, 'train_loss': 0.562542522418868, 'epoch': 2.0}

======================================================================
âœ… Training Completed Successfully!
======================================================================
ğŸ’¾ Saving final model to /lustre/results/qwen-wikitext-20251129-201659...
âœ“ Model saved to: /lustre/results/qwen-wikitext-20251129-201659
âœ“ Training info saved to: /lustre/results/qwen-wikitext-20251129-201659/training_info.txt
ğŸ“Š View logs: tensorboard --logdir=/lustre/logs/qwen-wikitext-20251129-201659
ğŸ“¤ Results will sync to S3 (if configured)
======================================================================

[Rank 0] Training job finished successfully! ğŸ‰
[Rank 1] Training job finished successfully! ğŸ‰
compute-gpu-st-distributed-ml-2:7070:11027 [0] NCCL INFO misc/socket.cc:47 -> 3
compute-gpu-st-distributed-ml-2:7070:11027 [0] NCCL INFO misc/socket.cc:58 -> 3
compute-gpu-st-distributed-ml-2:7070:11027 [0] NCCL INFO misc/socket.cc:781 -> 3
compute-gpu-st-distributed-ml-2:7070:7147 [0] NCCL INFO misc/socket.cc:832 -> 3
compute-gpu-st-distributed-ml-1:7614:9432 [0] NCCL INFO misc/socket.cc:47 -> 3
compute-gpu-st-distributed-ml-1:7614:9432 [0] NCCL INFO misc/socket.cc:58 -> 3
compute-gpu-st-distributed-ml-1:7614:9432 [0] NCCL INFO misc/socket.cc:781 -> 3
compute-gpu-st-distributed-ml-1:7614:7692 [0] NCCL INFO misc/socket.cc:832 -> 3
compute-gpu-st-distributed-ml-2:7070:11027 [0] NCCL INFO comm 0x60630f7f62f0 rank 1 nranks 2 cudaDev 0 busId 1e0 - Abort COMPLETE
compute-gpu-st-distributed-ml-1:7614:9432 [0] NCCL INFO comm 0x573f9cb382e0 rank 0 nranks 2 cudaDev 0 busId 1e0 - Abort COMPLETE
==========================================
Training completed at Sat Nov 29 22:08:07 UTC 2025
Results saved to: /lustre/results/qwen-wikitext-20251129-201659/
Checkpoints: /lustre/checkpoints/qwen-wikitext-20251129-201659/
==========================================

```

#### íŠ¸ëŸ¬ë¸”ìŠˆíŒ…
ì•„ë˜ì™€ ê°™ì€ ê¶Œí•œ ë¬¸ì œë¡œ í•™ìŠµì´ ì¤‘ë‹¨ë˜ë©´ userë¥¼ `ubuntu`ë¡œ ë°”ê¾¸ê±°ë‚˜ chmodë¡œ ê¶Œí•œ ë³€ê²½ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤.
```bash
[Rank 0] Creating output directories...
âœ— [Rank 0] Error creating directories: [Errno 13] Permission denied: '/lustre/checkpoints/qwen-wikitext-20251202-142643'
```

```bash
# user ë³€ê²½
sudo chown -R $USER:$USER /lustre
```

### 3.3.1 ë¡œì»¬ ê²°ê³¼ í™•ì¸

#### ì²´í¬í¬ì¸íŠ¸ í™•ì¸

```bash
# ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ í™•ì¸
ls -lh /lustre/checkpoints/${EXPERIMENT_NAME}/

# ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡
find /lustre/checkpoints/${EXPERIMENT_NAME}/ -name "*.bin" -o -name "checkpoint-*"
```

**ì˜ˆìƒ ì¶œë ¥**:

```
ubuntu@ip-10-0-3-12:~$ # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ í™•ì¸
ubuntu@ip-10-0-3-12:~$ ls -lh /lustre/checkpoints/${EXPERIMENT_NAME}/
total 65K
drwxrwxr-x 2 ubuntu ubuntu 33K Nov 29 21:49 checkpoint-1486
drwxrwxr-x 2 ubuntu ubuntu 33K Nov 29 22:07 checkpoint-2972
ubuntu@ip-10-0-3-12:~$ 
ubuntu@ip-10-0-3-12:~$ # ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡
ubuntu@ip-10-0-3-12:~$ find /lustre/checkpoints/${EXPERIMENT_NAME}/ -name "*.bin" -o -name "checkpoint-*"
/lustre/checkpoints/qwen-wikitext-20251129-201659/checkpoint-1486
/lustre/checkpoints/qwen-wikitext-20251129-201659/checkpoint-1486/training_args.bin
/lustre/checkpoints/qwen-wikitext-20251129-201659/checkpoint-2972
/lustre/checkpoints/qwen-wikitext-20251129-201659/checkpoint-2972/training_args.bin
```

#### ìµœì¢… ê²°ê³¼ í™•ì¸

```bash
EXPERIMENT_NAME=$(tail -3 distributed-training-${JOB_ID}.out | grep -oP '(?<=/)[\w-]+(?=/$)' | head -1)
echo "EXPERIMENT_NAME: $EXPERIMENT_NAME"

# ê²°ê³¼ ë””ë ‰í† ë¦¬ í™•ì¸
ls -lh /lustre/results/${EXPERIMENT_NAME}/

# í•™ìŠµ ì •ë³´ í™•ì¸
cat /lustre/results/${EXPERIMENT_NAME}/training_info.txt
```

**ì˜ˆìƒ ì¶œë ¥:**
```
ubuntu@ip-10-0-3-12:~$ # ê²°ê³¼ ë””ë ‰í† ë¦¬ í™•ì¸
ubuntu@ip-10-0-3-12:~$ ls -lh /lustre/results/${EXPERIMENT_NAME}/
total 947M
-rw-rw-r-- 1 ubuntu ubuntu  605 Nov 29 22:07 added_tokens.json
-rw-rw-r-- 1 ubuntu ubuntu 2.4K Nov 29 22:07 chat_template.jinja
-rw-rw-r-- 1 ubuntu ubuntu 1.3K Nov 29 22:07 config.json
-rw-rw-r-- 1 ubuntu ubuntu  117 Nov 29 22:07 generation_config.json
-rw-rw-r-- 1 ubuntu ubuntu 1.6M Nov 29 22:07 merges.txt
-rw-rw-r-- 1 ubuntu ubuntu 943M Nov 29 22:07 model.safetensors
-rw-rw-r-- 1 ubuntu ubuntu  502 Nov 29 22:07 special_tokens_map.json
-rw-rw-r-- 1 ubuntu ubuntu  11M Nov 29 22:07 tokenizer.json
-rw-rw-r-- 1 ubuntu ubuntu 4.6K Nov 29 22:07 tokenizer_config.json
-rw-rw-r-- 1 ubuntu ubuntu 5.4K Nov 29 22:07 training_args.bin
-rw-rw-r-- 1 ubuntu ubuntu  238 Nov 29 22:07 training_info.txt
-rw-rw-r-- 1 ubuntu ubuntu 2.7M Nov 29 22:07 vocab.json
ubuntu@ip-10-0-3-12:~$ 
ubuntu@ip-10-0-3-12:~$ # í•™ìŠµ ì •ë³´ í™•ì¸
ubuntu@ip-10-0-3-12:~$ cat /lustre/results/${EXPERIMENT_NAME}/training_info.txt
Experiment Name: qwen-wikitext-20251129-201659
Dataset: WikiText-2
Model: Qwen2.5-0.5B
Nodes: 2
Total Samples: 23767
Epochs: 2
Batch Size (per device): 4
Gradient Accumulation: 2
Learning Rate: 2e-05
Completed: 2025-11-29T22:07:56.257921
```
</details>

âœ… ë¶„ì‚° í•™ìŠµ ì‹¤í–‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!


---

## ğŸ“š ë„¤ë¹„ê²Œì´ì…˜

| ì´ì „ | ìƒìœ„ | ë‹¤ìŒ |
|------|------|------|
| [â—€ í´ëŸ¬ìŠ¤í„° ë°°í¬](./02-pcluster-deployment.md) | [ğŸ“‘ ëª©ì°¨](../README.md#-ê°€ì´ë“œ-ëª©ì°¨) | Coming Soon |
