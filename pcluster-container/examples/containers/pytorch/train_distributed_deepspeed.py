"""
Multi-node Distributed Training Script for AWS ParallelCluster
Uses /scratch for HuggingFace caching to avoid permission issues
"""

import os
import socket
import torch
import torch.distributed as dist
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
from datasets import load_from_disk
from datetime import datetime
import logging

# ============================================
# ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
# ============================================
def setup_cache_dirs():
    """Set up cache directories, preferring /scratch over /tmp"""
    # /scratchê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ /tmp ì‚¬ìš©
    if os.path.exists("/scratch"):
        CACHE_BASE = "/scratch"
        print("âœ“ Using /scratch for caching (NVMe - Fast!)")
    else:
        CACHE_BASE = "/tmp"
        print("âš  /scratch not found, using /tmp for caching")
    
    # ì‚¬ìš©ìë³„ ìºì‹œ ë””ë ‰í† ë¦¬
    uid = os.getuid()
    HF_CACHE = f"{CACHE_BASE}/hf_cache_{uid}"
    TRANSFORMERS_CACHE = f"{CACHE_BASE}/transformers_cache_{uid}"
    TRITON_CACHE = f"{CACHE_BASE}/triton_cache_{uid}"
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(HF_CACHE, exist_ok=True)
    os.makedirs(TRANSFORMERS_CACHE, exist_ok=True)
    os.makedirs(TRITON_CACHE, exist_ok=True)
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    os.environ['HF_DATASETS_CACHE'] = HF_CACHE
    os.environ['TRANSFORMERS_CACHE'] = TRANSFORMERS_CACHE
    os.environ['TRITON_CACHE_DIR'] = TRITON_CACHE
    os.environ['HF_HOME'] = HF_CACHE
    
    print(f"ğŸ“¦ Cache directories:")
    print(f"   HF_DATASETS_CACHE: {HF_CACHE}")
    print(f"   TRANSFORMERS_CACHE: {TRANSFORMERS_CACHE}")
    print(f"   TRITON_CACHE_DIR: {TRITON_CACHE}")
    
    return HF_CACHE, TRANSFORMERS_CACHE

# ============================================
# ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ í•¨ìˆ˜
# ============================================
def prepare_dataset(examples, tokenizer, max_length=512):
    """Tokenize text data"""
    tokenized = tokenizer(
        examples["text"],
        truncation=True,
        max_length=max_length,
        padding="max_length",
        return_tensors=None
    )
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

# ============================================
# ë©”ì¸ í•™ìŠµ í•¨ìˆ˜
# ============================================
def main():
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='[%(asctime)s] [Rank %(rank)s] %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # ============================================
    # 1. ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì • (ê°€ì¥ ë¨¼ì €!)
    # ============================================
    HF_CACHE, TRANSFORMERS_CACHE = setup_cache_dirs()
    
    # ============================================
    # 2. ë¶„ì‚° í™˜ê²½ ì´ˆê¸°í™”
    # ============================================
    local_rank = int(os.environ.get('LOCAL_RANK', 0))
    world_size = int(os.environ.get('WORLD_SIZE', 1))
    rank = int(os.environ.get('RANK', 0))
    
    # PyTorch ë¶„ì‚° ì´ˆê¸°í™”
    if not dist.is_initialized():
        dist.init_process_group(backend='nccl')
        torch.cuda.set_device(local_rank)
    
    # ë…¸ë“œ ì •ë³´
    node_name = socket.gethostname()
    
    # ë¡œê¹…ì— rank ì¶”ê°€
    logging.LoggerAdapter(logging.getLogger(), {'rank': rank})
    
    # ============================================
    # 3. ê²½ë¡œ ì„¤ì •
    # ============================================
    lustre_base = os.environ.get('LUSTRE_BASE', '/lustre')
    experiment_name = os.environ.get(
        'EXPERIMENT_NAME', 
        f'qwen-wikitext-{datetime.now().strftime("%Y%m%d-%H%M%S")}'
    )
    
    # ë°ì´í„° ê²½ë¡œ
    data_path = f"{lustre_base}/data/wikitext-2"
    
    # ì¶œë ¥ ê²½ë¡œ
    checkpoint_path = f"{lustre_base}/checkpoints/{experiment_name}"
    log_path = f"{lustre_base}/logs/{experiment_name}"
    result_path = f"{lustre_base}/results/{experiment_name}"
    
    print("\n" + "="*70)
    print(f"ğŸš€ Distributed Training Starting")
    print("="*70)
    print(f"ğŸ“ Node: {node_name}")
    print(f"ğŸ¯ Rank: {rank}/{world_size} (Local Rank: {local_rank})")
    print(f"ğŸ“‚ Dataset: {data_path}")
    print(f"ğŸ’¾ Checkpoints: {checkpoint_path}")
    print(f"ğŸ“Š Logs: {log_path}")
    print(f"ğŸ¯ Results: {result_path}")
    print(f"ğŸ”§ Experiment: {experiment_name}")
    print("="*70 + "\n")
    
    # ============================================
    # 4. ë””ë ‰í† ë¦¬ ìƒì„± (Rank 0ë§Œ)
    # ============================================
    if rank == 0:
        print(f"[Rank 0] Creating output directories...")
        try:
            os.makedirs(checkpoint_path, exist_ok=True)
            os.makedirs(log_path, exist_ok=True)
            os.makedirs(result_path, exist_ok=True)
            
            # ì“°ê¸° í…ŒìŠ¤íŠ¸
            test_file = os.path.join(checkpoint_path, ".write_test")
            with open(test_file, "w") as f:
                f.write("test")
            os.remove(test_file)
            print(f"âœ“ [Rank 0] Output directories created and writable")
            
        except Exception as e:
            print(f"âœ— [Rank 0] Error creating directories: {e}")
            raise
    
    # ëª¨ë“  rankê°€ ë””ë ‰í† ë¦¬ ìƒì„± ëŒ€ê¸°
    dist.barrier()
    print(f"[Rank {rank}] Synchronized, proceeding...")
    
    # ============================================
    # 5. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    # ============================================
    print(f"[Rank {rank}] Loading Qwen2.5-0.5B model...")
    model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-0.5B",
        attn_implementation="flash_attention_2",
        torch_dtype=torch.bfloat16,
        cache_dir=TRANSFORMERS_CACHE,
    )
    
    print(f"[Rank {rank}] Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        "Qwen/Qwen2.5-0.5B",
        cache_dir=TRANSFORMERS_CACHE,
    )
    tokenizer.pad_token = tokenizer.eos_token
    
    # ============================================
    # 6. ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬
    # ============================================
    print(f"[Rank {rank}] Loading WikiText-2 dataset from {data_path}...")
    
    if not os.path.exists(data_path):
        raise FileNotFoundError(
            f"Dataset not found at {data_path}. "
            f"Please ensure FSx Lustre is mounted and synced."
        )
    
    dataset = load_from_disk(data_path)
    train_dataset = dataset["train"]
    
    print(f"[Rank {rank}] Original dataset size: {len(train_dataset)}")
    
    # ë¹ˆ í…ìŠ¤íŠ¸ í•„í„°ë§ (ìºì‹œë¥¼ ë¡œì»¬ì— ì €ì¥)
    print(f"[Rank {rank}] Filtering empty texts...")
    cache_file_filter = f"{HF_CACHE}/filtered_train_rank{rank}.arrow"
    
    train_dataset = train_dataset.filter(
        lambda x: len(x["text"]) > 0 and not x["text"].isspace(),
        cache_file_name=cache_file_filter,
        desc=f"Filtering (Rank {rank})"
    )
    
    print(f"[Rank {rank}] Filtered dataset size: {len(train_dataset)}")
    
    # í† í°í™” (ìºì‹œë¥¼ ë¡œì»¬ì— ì €ì¥)
    print(f"[Rank {rank}] Tokenizing dataset...")
    cache_file_tokenize = f"{HF_CACHE}/tokenized_train_rank{rank}.arrow"
    
    tokenized_dataset = train_dataset.map(
        lambda x: prepare_dataset(x, tokenizer),
        batched=True,
        remove_columns=train_dataset.column_names,
        desc=f"Tokenizing (Rank {rank})",
        num_proc=4,
        cache_file_name=cache_file_tokenize,
    )
    
    print(f"[Rank {rank}] Tokenization complete. Dataset size: {len(tokenized_dataset)}")
    
    # ============================================
    # 7. í•™ìŠµ ì„¤ì •
    # ============================================
    print(f"[Rank {rank}] Setting up training configuration...")
    
    training_args = TrainingArguments(
        # ì¶œë ¥ ê²½ë¡œ
        output_dir=checkpoint_path,
        logging_dir=log_path,
        
        # í•™ìŠµ ì„¤ì •
        num_train_epochs=2,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=2,
        
        # ì˜µí‹°ë§ˆì´ì €
        learning_rate=2e-5,
        warmup_steps=100,
        weight_decay=0.01,
        
        # ë¡œê¹…
        logging_steps=10,
        logging_first_step=True,
        report_to="tensorboard",
        
        # ì²´í¬í¬ì¸íŠ¸
        save_strategy="epoch",
        save_total_limit=2,
        
        # ë¶„ì‚° í•™ìŠµ
        ddp_backend="nccl",
        local_rank=local_rank,
        
        # í˜¼í•© ì •ë°€ë„
        bf16=True,
        
        # ê¸°íƒ€
        dataloader_num_workers=4,
        dataloader_pin_memory=True,
    )
    
    # Trainer ì´ˆê¸°í™”
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
    )
    
    # ============================================
    # 8. í•™ìŠµ ì‹œì‘
    # ============================================
    print("\n" + "="*70)
    print(f"ğŸ‹ï¸  [Rank {rank}] Starting Distributed Training")
    print("="*70)
    print(f"ğŸ“Š Total Samples: {len(tokenized_dataset)}")
    print(f"ğŸ”¢ Batch Size per Device: {training_args.per_device_train_batch_size}")
    print(f"ğŸ”„ Gradient Accumulation Steps: {training_args.gradient_accumulation_steps}")
    print(f"ğŸŒ World Size: {world_size}")
    print(f"ğŸ“ˆ Effective Batch Size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps * world_size}")
    print(f"ğŸ“ Logs: tensorboard --logdir={log_path}")
    print("="*70 + "\n")
    
    # í•™ìŠµ ì‹¤í–‰
    trainer.train()
    
    # ============================================
    # 9. í•™ìŠµ ì™„ë£Œ ë° ì €ì¥ (Rank 0ë§Œ)
    # ============================================
    if rank == 0:
        print("\n" + "="*70)
        print("âœ… Training Completed Successfully!")
        print("="*70)
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        print(f"ğŸ’¾ Saving final model to {result_path}...")
        trainer.save_model(result_path)
        tokenizer.save_pretrained(result_path)
        
        # í•™ìŠµ ì •ë³´ ì €ì¥
        info_file = os.path.join(result_path, "training_info.txt")
        with open(info_file, "w") as f:
            f.write(f"Experiment Name: {experiment_name}\n")
            f.write(f"Dataset: WikiText-2\n")
            f.write(f"Model: Qwen2.5-0.5B\n")
            f.write(f"Nodes: {world_size}\n")
            f.write(f"Total Samples: {len(tokenized_dataset)}\n")
            f.write(f"Epochs: {training_args.num_train_epochs}\n")
            f.write(f"Batch Size (per device): {training_args.per_device_train_batch_size}\n")
            f.write(f"Gradient Accumulation: {training_args.gradient_accumulation_steps}\n")
            f.write(f"Learning Rate: {training_args.learning_rate}\n")
            f.write(f"Completed: {datetime.now().isoformat()}\n")
        
        print(f"âœ“ Model saved to: {result_path}")
        print(f"âœ“ Training info saved to: {info_file}")
        print(f"ğŸ“Š View logs: tensorboard --logdir={log_path}")
        print(f"ğŸ“¤ Results will sync to S3 (if configured)")
        print("="*70 + "\n")
    
    # ìµœì¢… ë™ê¸°í™”
    dist.barrier()
    
    print(f"[Rank {rank}] Training job finished successfully! ğŸ‰")

if __name__ == "__main__":
    main()