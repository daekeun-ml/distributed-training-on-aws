import os
import socket
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
from datasets import load_from_disk
from datetime import datetime

os.makedirs("/root/.triton/autotune", exist_ok=True)

def prepare_dataset(examples, tokenizer, max_length=512):
    """í…ìŠ¤íŠ¸ë¥¼ í† í°í™”"""
    tokenized = tokenizer(
        examples["text"],
        truncation=True,
        max_length=max_length,
        padding="max_length",
        return_tensors=None
    )
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

def main():
    # ë¶„ì‚° í™˜ê²½ ì •ë³´
    local_rank = int(os.environ.get('LOCAL_RANK', 0))
    world_size = int(os.environ.get('WORLD_SIZE', 1))
    rank = int(os.environ.get('RANK', 0))
    
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ ë…¸ë“œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    node_name = os.environ.get('NODE_NAME', socket.gethostname())
    
    # FSx Lustre ê²½ë¡œ ì„¤ì •
    lustre_base = os.environ.get('LUSTRE_BASE', '/lustre')
    experiment_name = os.environ.get('EXPERIMENT_NAME', f'qwen-wikitext-{datetime.now().strftime("%Y%m%d-%H%M%S")}')
    
    # ê²½ë¡œ ì •ì˜
    data_path = f"{lustre_base}/data/wikitext-2"
    checkpoint_path = f"{lustre_base}/checkpoints/{experiment_name}"
    log_path = f"{lustre_base}/logs/{experiment_name}"
    result_path = f"{lustre_base}/results/{experiment_name}"
    
    print(f"ğŸš€ [Rank {rank}/{world_size}] {node_name} GPU {local_rank} - Starting...")
    print(f"ğŸ“‚ [Rank {rank}] Dataset: {data_path}")
    print(f"ğŸ’¾ [Rank {rank}] Checkpoints: {checkpoint_path}")
    print(f"ğŸ“Š [Rank {rank}] Logs: {log_path}")
    print(f"ğŸ¯ [Rank {rank}] Results: {result_path}")
    
    # ë””ë ‰í† ë¦¬ ìƒì„± (rank 0ë§Œ)
    if rank == 0:
        os.makedirs(checkpoint_path, exist_ok=True)
        os.makedirs(log_path, exist_ok=True)
        os.makedirs(result_path, exist_ok=True)
    
    # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    print(f"[Rank {rank}] Loading Qwen 0.5B...")
    model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-0.5B",
        attn_implementation="flash_attention_2",
        torch_dtype=torch.bfloat16,
    )
    
    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B")
    tokenizer.pad_token = tokenizer.eos_token
    
    # FSx Lustreì—ì„œ WikiText-2 ë¡œë“œ
    print(f"[Rank {rank}] Loading WikiText-2 from FSx Lustre...")
    if not os.path.exists(data_path):
        raise FileNotFoundError(
            f"Dataset not found at {data_path}. "
            f"Please ensure FSx Lustre is synced with S3."
        )
    
    dataset = load_from_disk(data_path)
    train_dataset = dataset["train"]
    
    # ë¹ˆ í…ìŠ¤íŠ¸ ì œê±°
    print(f"[Rank {rank}] Filtering empty texts...")
    train_dataset = train_dataset.filter(
        lambda x: len(x["text"]) > 0 and not x["text"].isspace()
    )
    
    print(f"[Rank {rank}] Tokenizing dataset... (Total samples: {len(train_dataset)})")
    tokenized_dataset = train_dataset.map(
        lambda x: prepare_dataset(x, tokenizer),
        batched=True,
        remove_columns=train_dataset.column_names,
        desc="Tokenizing",
        num_proc=4
    )
    
    print(f"[Rank {rank}] Setting up DeepSpeed training...")
    
    training_args = TrainingArguments(
        output_dir=checkpoint_path,
        logging_dir=log_path,
        num_train_epochs=2,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=2,
        logging_steps=10,
        bf16=True,
        learning_rate=2e-5,
        warmup_steps=10,
        save_strategy="epoch",
        save_total_limit=2,
        report_to="tensorboard",
        logging_first_step=True,
        deepspeed="ds_config.json",
        ddp_backend="nccl",
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
    )
    
    print(f"ğŸ‹ï¸  [Rank {rank}] Starting distributed training on WikiText-2...")
    print(f"ğŸ“ˆ [Rank {rank}] TensorBoard logs: {log_path}")
    
    trainer.train()
    
    if rank == 0:
        print("\n" + "="*60)
        print("âœ… Multi-node DeepSpeed training completed!")
        print("ğŸ‰ WikiText-2 training successful!")
        print("="*60)
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        print(f"ğŸ’¾ Saving final model to {result_path}...")
        trainer.save_model(result_path)
        tokenizer.save_pretrained(result_path)
        
        # í•™ìŠµ ì •ë³´ ì €ì¥
        with open(f"{result_path}/training_info.txt", "w") as f:
            f.write(f"Experiment: {experiment_name}\n")
            f.write(f"Dataset: WikiText-2\n")
            f.write(f"Nodes: {world_size}\n")
            f.write(f"Total samples: {len(train_dataset)}\n")
            f.write(f"Epochs: 2\n")
            f.write(f"Completed: {datetime.now()}\n")
        
        print(f"âœ… Final model saved to: {result_path}")
        print(f"ğŸ“¤ Results will sync to S3: s3://your-bucket/results/{experiment_name}/")
        print(f"ğŸ“Š View logs: tensorboard --logdir={log_path}")

if __name__ == "__main__":
    main()